{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import pickle\n",
    "import gzip\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lychee():\n",
    "    def __init__(self, \n",
    "                 mode = 'cor',\n",
    "                 lm_order = 3, \n",
    "                 lm_counts = 'default', \n",
    "                 c_dict = 'default',\n",
    "                 check_all=False, \n",
    "                 lm_lambdas = [0.2, 0.3, 0.5], \n",
    "                 threshold=4):\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.lm = self.LanguageModel(order = lm_order, lambdas = lm_lambdas, counts = lm_counts)\n",
    "        self.c = self.Candidator(self.lm.counts['n1'] if c_dict == 'default' else c_dict, \n",
    "                                 check_all = check_all)\n",
    "        \n",
    "    def check_pattern(self, pattern):\n",
    "        '''генерирует все варианты для полученной строки на основе комбинаций кандидатов\n",
    "        и проверяет их с помощью модели языка, выбирает наилучший вариант'''\n",
    "        cand_list = self.c.candidates(pattern)\n",
    "        cand_probs = sorted([(cand, self.lm.prob(cand, log=True)) for cand in cand_list], \n",
    "                key=lambda x:x[1], reverse=True)\n",
    "        if self.mode == 'cor':\n",
    "            return cand_probs[0][0]\n",
    "        elif self.mode == 'prob':\n",
    "            return cand_probs[:10]   \n",
    "\n",
    "        \n",
    "    def check(self, text):   \n",
    "        corrected_text = []\n",
    "        for sent in sent_tokenize(text):\n",
    "            corrected_sent = []\n",
    "            tmp = []\n",
    "            words = Lychee.cleanse(sent).split()\n",
    "            for i, word in enumerate(words):\n",
    "                if len(tmp) < self.threshold-1 and i != len(words)-1:\n",
    "                    tmp.append(word)\n",
    "                else: \n",
    "                    tmp.append(word)\n",
    "                    corrected_sent.append(self.check_pattern(' '.join(tmp)))\n",
    "                    tmp = []\n",
    "            corrected_text.append(' '.join(corrected_sent))\n",
    "        return corrected_text\n",
    "                    \n",
    "    \n",
    "    class LanguageModel():\n",
    "        def __init__(self, order, lambdas, counts=None):\n",
    "            self.order = order\n",
    "            self.lambdas = lambdas\n",
    "            if counts == 'default':\n",
    "                print('... please wait ... loading model ...')\n",
    "                self.counts = pickle.load(gzip.open('default.pkl.gz'))\n",
    "            else: self.counts = counts\n",
    "\n",
    "        def product(self, nums):\n",
    "            \"возвращает произведение чисел\"\n",
    "            result = 1\n",
    "            for x in nums: result *= x\n",
    "            return result\n",
    "\n",
    "        def get_ngrams(self, tokens, n):\n",
    "            '''группирует на н-граммы список токенов (слов)'''\n",
    "            return [' '.join(tokens[i:i+n]) for i, token in enumerate(tokens)]\n",
    "\n",
    "        def get_counts(self, corpus, order):  \n",
    "            '''создает набор частотных словарей порядка от 0 до заданного \n",
    "            на основе списка токенов'''\n",
    "            counts = {'n' + str(i) : Counter(self.get_ngrams(corpus, n=i)) \\\n",
    "                      for i in range(1, order+1)}\n",
    "            counts['n0'] = {'':len(corpus)}\n",
    "            return counts\n",
    "\n",
    "        def get_prob(self, counts, word, context=''):\n",
    "            '''возвращает вероятность токена (слова или нграммы)'''\n",
    "            order = len(context.split())+1\n",
    "            separator = ' ' if order > 1 else ''\n",
    "            return (counts['n'+str(order)][separator.join([context, word])] + 1) / \\\n",
    "                   (counts['n'+str(order-1)][context] + len(counts['n'+str(order)]))\n",
    "\n",
    "        def get_logprob(self, counts, word, context=''):\n",
    "            '''возвращает логарифмическую вероятность токена'''\n",
    "            return np.log(self.get_prob(counts, word, context))\n",
    "\n",
    "        def get_following(self, counts, context):\n",
    "            '''возвращает список слов, следующих за контекстом, с частотами и вероятностями'''\n",
    "            order = len(context.split())+1\n",
    "            return sorted(\n",
    "                [(k.split()[-1], v, self.get_prob(counts, k.split()[-1], context)) \\\n",
    "                for k, v in counts['n'+str(order)].items()                         \\\n",
    "                if re.match(context+' '+'\\w+', k)],                                \\\n",
    "                key=lambda x:x[1], reverse=True)   \n",
    "\n",
    "        def get_string_probs(self, counts, string, order, log=True):\n",
    "            '''возвращает вероятность набора слов любой длины'''\n",
    "            prob_fun = self.get_logprob if log else self.get_prob\n",
    "            tokens = string.split()\n",
    "            probs = []\n",
    "            for i in range(len(tokens)):\n",
    "                context = ' '.join(tokens[i-order+1:i]) if i>=order else ' '.join(tokens[:i])\n",
    "                prob = prob_fun(counts, word = tokens[i], context = context)\n",
    "                probs.append(prob)\n",
    "            return probs\n",
    "\n",
    "        def interpolate(self, counts, string, order, log=True, lambdas='default'):\n",
    "            '''возвращает интерполированную вероятность набора слов любой длины,\n",
    "            комбинируя вероятности, расчитыванные на нграммах разной длины,\n",
    "            с весами из аргумента lambdas'''\n",
    "            lmbd = self.lambdas if lambdas == 'default' else lambdas\n",
    "            aggregate = sum if log else self.product\n",
    "            probs = [self.get_string_probs(counts, string, order=i, log=log) \\\n",
    "                     for i in range(1, order+1)]\n",
    "            probs_interpolated = []\n",
    "            for tup in zip(*probs):\n",
    "                prob_token = 0\n",
    "                for i in range(len(tup)):\n",
    "                    prob_token += tup[i] * lmbd[i]\n",
    "                probs_interpolated.append(prob_token)\n",
    "            return aggregate(probs_interpolated)\n",
    "\n",
    "        def fit(self, corpus):\n",
    "            '''расчет и сохранение модели'''\n",
    "            self.counts = self.get_counts(corpus, self.order)\n",
    "\n",
    "        def prob(self, string, log=False):\n",
    "            '''возвращает конечную (интерполированную) вероятность \n",
    "            для произвольного работа слов любой длины, \"публичный\" метод'''\n",
    "            return self.interpolate(self.counts, string, self.order, log=log)\n",
    "\n",
    "        def context_prob(self, word, context='', log=False):\n",
    "            '''возвращает вероятность того, что данное слово следует за данным контекстом'''\n",
    "            prob_fun = self.get_logprob if log else self.get_prob\n",
    "            c = context.split()\n",
    "            history = ' '.join(c) if len(c) < self.order else ' '.join(c[-self.order+1:])\n",
    "            return prob_fun(self.counts, word, history)  \n",
    "\n",
    "        def following(self, context):\n",
    "            '''возвращает список слов, следующих за контекстом, \n",
    "            с частотами и вероятностями, \"публичный\" метод'''\n",
    "            c = context.split()\n",
    "            history = ' '.join(c) if len(c) < self.order else ' '.join(c[-self.order+1:])\n",
    "            return self.get_following(self.counts, history)\n",
    "\n",
    "    class Candidator():\n",
    "        def __init__(self, dictionary, abc='йцукенгшщзхъфывапролджэячсмитьбю-', check_all=False):\n",
    "            self.abc = abc\n",
    "            self.dictionary = set(dictionary)\n",
    "            self.check_all = check_all\n",
    "\n",
    "        def edits(self, word): # mostly from norvig, modified for faster dict search\n",
    "            '''возвращает множество кандидатов (редакций) для отдельного слова,\n",
    "            сохраняются только те редакции, которые есть в словаре'''\n",
    "            letters = self.abc\n",
    "            d = self.dictionary\n",
    "            splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "            deletes = [ed for ed in [L + R[1:] for L, R in splits if R] if ed in d]\n",
    "            transposes = [ed for ed in [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1] \\\n",
    "                          if ed in d]\n",
    "            replaces = [ed for ed in [L + c + R[1:] for L, R in splits if R for c in letters] \\\n",
    "                        if ed in d]\n",
    "            inserts = [ed for ed in [L + c + R for L, R in splits for c in letters] if ed in d]\n",
    "            return set(deletes + transposes + replaces + inserts + [word])\n",
    "\n",
    "        def get_sentences(self, text):\n",
    "            '''делит текст на предложения'''\n",
    "            return text.lower().split('. ')\n",
    "\n",
    "        def word_candidates(self, sent):\n",
    "            '''для всех или только для неизвестных слов в предложении \n",
    "            возвращает список кандидатов'''\n",
    "            if self.check_all: return [self.edits(word) for word in sent.lower().split()]\n",
    "            else: return [self.edits(word) if (word not in self.dictionary) else {word}\\\n",
    "                        for word in sent.lower().split()]\n",
    "\n",
    "        def sent_candidates(self, sent):\n",
    "            '''генерирует множество кандидатов для всего предложения'''\n",
    "            return {candidate for candidate in product(*self.word_candidates(sent))}\n",
    "\n",
    "        def candidates(self, sent):\n",
    "            '''возвращает множество кандидатов-предложений в виде текста'''\n",
    "            return {' '.join(candidate) for candidate in self.sent_candidates(sent)}\n",
    "\n",
    "    class Ranker():\n",
    "        def __init__(self, lang_model, candidator):\n",
    "            self.lm = lang_model\n",
    "            self.c = candidator\n",
    "            \n",
    "    def cleanse(s):\n",
    "        rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«\\»№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "        return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... please wait ... loading model ...\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lychee = Lychee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.67 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['вкусный суп с мясом',\n",
       " 'гжегож бженчишчикевич',\n",
       " 'володя дурак',\n",
       " 'опечатка в слове опечатка не опечатка',\n",
       " 'моя программа работает просто дурдом']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "text = '. '.join(['вкучный свуп c мэсом', \n",
    "          'гжегож бженчишчикевич', \n",
    "          'влодя дуурак', \n",
    "          'оппечатка в злове упечатка не опечатко', \n",
    "          'ммоя програма работаит паросто дур-дом'])\n",
    "lychee.check(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training.txt', encoding='utf-8') as f:\n",
    "    pairs = [[s for s in pair.split('\\n')] for pair in f.read().split('\\n\\n')]\n",
    "\n",
    "wrong = [pair[0] for pair in pairs]\n",
    "correct = [pair[1].lower() for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:07<00:00, 382.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251 из 3000(41.7%) предложений полностью исправлены\n",
      "Wall time: 7.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lycheed = [lychee.check(sent) for sent in tqdm(wrong)]\n",
    "total = 0\n",
    "hooray = 0\n",
    "for pair in zip(lycheed, correct):\n",
    "    total += 1\n",
    "    if pair[0] == [pair[1]]:\n",
    "        hooray += 1     \n",
    "print('{} из {}({:.1%}) предложений полностью исправлены'.format(hooray, total, hooray/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
